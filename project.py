# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uXFgR8iCGFBKWtmw6psqC5cYj0oOatg5
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import matplotlib as mpl 
import seaborn as sns

import plotly 
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as py
from plotly.offline import iplot
from plotly.subplots import make_subplots
import plotly.figure_factory as ff


import warnings
warnings.filterwarnings('ignore')

!pip install -U plotly

"""# Files Uploaded & Dataframes Created

"""

from google.colab import files
import io
uploaded = files.upload()
simply = pd.read_csv('simplyhired.csv')

from google.colab import files
import io
uploaded = files.upload()
dice2 = pd.read_csv('dice2.csv')

from google.colab import files
import io
uploaded = files.upload()
dice = pd.read_csv('dice.csv')

dice.head(0)

import csv 
import json
data  = json.load(open('dice.json'))
json = pd.DataFrame(data["data"])
json.head(0)

import numpy as np
import pandas as pd

all_columns = list(dice.columns)  + list(simply.columns)
all_unique_columns = np.unique(np.array(all_columns)) # this will, as the name suggests, give an end list of just the unique columns. You could run print(all_unique_columns) to make sure it has what you want
df = pd.DataFrame(columns=all_unique_columns)
df = pd.concat([dice, simply],axis=1)
df.head()

"""# Json File Cleaned 

"""

json.sort_values(by=['salary'], ascending=False)

json.rename(columns={'postedDate': 'jobpostdate', 'workFromHomeAvailability': 'workFromHomeavailability'}, inplace=True)

json['salary'] = json['salary'].replace(['$'],'Unknown')

json.replace('-1', np.nan, inplace=True)
json.replace(-1, np.nan, inplace=True)
json.replace(-1.0, np.nan, inplace=True)

json.isnull().sum()

json['salary'] = json['salary'].str.replace(',','-')

json.drop('detailsPageUrl', axis=1, inplace=True)
json.drop('companyPageUrl', axis=1, inplace=True)
json.drop('companyLogoUrl', axis=1, inplace=True) 
json.drop('id', axis=1, inplace=True)
json.drop('clientBrandId', axis=1, inplace=True)
json.drop('jobId', axis=1, inplace=True)

import ast
json['jobLocation'] = json['jobLocation'].dropna().astype('str').apply(ast.literal_eval).str['displayName'] 
#got rid of the dictionary style

"""# Json Statistical Analysis"""

json.head(0)

json['title'].value_counts().head(10)

json['salary'].value_counts()

json.salary.describe()

json.sort_values(by=['salary'], ascending=False)

json.companyName.value_counts()

json.loc[json['companyName'] == 'Discover'] #theres duplicate but the jobs were posted at different times

json.loc[json['companyName'] == 'Deloitte']

json.sort_values(by=['score'], ascending=False)

json[['companyName', 'score']].sample(10)

json.groupby(['easyApply'])['title'].count()

json.jobLocation.describe()

json.employmentType.describe()

json.employmentType.value_counts()

json.groupby(['employmentType'])['title'].count()

json.groupby(['salary'])['title'].count()

json.title.describe()

json.title.value_counts()

top10 = json.jobLocation.value_counts()[:10]

json.head(0)

json[['salary', 'title', 'companyName']]

"""# json Graphs"""

json.jobLocation.value_counts()[:10].plot(kind='bar', stacked= True, colormap = 'Purples_r' )

import matplotlib.pyplot as plt
import seaborn as sns

pd.crosstab(json['isRemote'],json['easyApply']).plot(kind = 'bar', stacked=False , colormap = 'cool_r').set_title("Applying to Remote Jobs")

unstacked = json.groupby(['isRemote', 'easyApply']).size().unstack()
unstacked.plot(kind='bar', stacked=True , colormap = 'cool_r')

temp = json.companyName.value_counts()
temp2 = temp.head(10)
if len(temp) > 10:
    temp2['remaining {0} items'.format(len(temp) - 10)] = sum(temp[10:])
temp2.plot(kind='pie', colormap='cool')

top_colors = json.companyName.value_counts()
top_colors[:10].plot(kind='barh',stacked=True, colormap='Purples_r')
plt.xlabel('No. of job openings by company')
plt.title("Companys Hiring")
colormap='Paired'

fig = go.Figure()

fig.add_trace(go.Bar(x=json['title'].value_counts().head(20).index,
       y=json['title'].value_counts().head(20).values,
       marker_color = 'fuchsia'))

fig.update_layout(
    
    height=600, width=800, title_text='Number of job openings by Job Titles',
    
    xaxis_title='job title', yaxis_title="count", title_x = 0.5,
    
    font=dict(
            family="Courier New, monospace",
            size=14,
            color="black"
        
))


fig.show()
##330C73

!apt install proj-bin libproj-dev libgeos-dev
!apt install libgeos-3.5.0
!pip install https://github.com/matplotlib/basemap/archive/master.zip

json1 = json['jobLocation'].apply(pd.Series)

count = json.jobLocation.value_counts()

json['state'] = json['jobLocation'].str.split(', ').str[1]

fig = px.choropleth(json.dropna(subset=['state']), 
                    locations='state', 
                    locationmode='USA-states', 
                    color='jobLocation',
                    scope='usa',
                    labels={'state':'jobLocation'})
fig.show()

import pandas as pd
import matplotlib.pyplot as plt

#import your data here

#Plot a histogram of frequencies
json.salary.value_counts().plot(kind='barh', stacked=False, colormap='Purples_r')
plt.title('Frequency of Salary Offered')
plt.xlabel('Frequency')

json.salary.value_counts().plot(kind='pie',  stacked=True, colormap='cool')
plt.axis('equal')
plt.title('Number of appearances in dataset')

json.employmentType.value_counts().plot(kind='pie',  stacked=True, colormap='cool')
plt.axis('equal')
plt.title('Frequency of Employment Type in dataset')

location = json["jobLocation"].apply(pd.Series)

json.head(10)

"""# Df Statistical Analysis"""

df.head(1)

simply.head()

simply['salary'].value_counts()

simply.salary.describe()

"""# DF Graphs

"""

from wordcloud import WordCloud, STOPWORDS 
import matplotlib.pyplot as plt 
import pandas as pd 

fields = ['title']
df1 = pd.read_csv('dice2.csv', usecols=fields)

text = df1['title'].values 

wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""# Queries"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get install openjdk-8-jdk-headless -qq > /dev/null
# [ ! -e "$(basename spark-3.1.2-bin-hadoop2.7.tgz)" ] && wget  http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz  
# tar xf spark-3.1.2-bin-hadoop2.7.tgz
# pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

! [ ! -e "$(basename simplyhired.csv)" ] && wget  https://storage.googleapis.com/mbcc/datasets/simplyhired.csv
sparkdf = spark.read.csv('simplyhired.csv',
                      header= True, 
                      inferSchema = True)
print(" The Simply Hired Spark dataframe has {} records".format(sparkdf.count()))

sparkdf.drop_duplicates()
sparkdf.count()

#Roughly 72.69% of all postings in dataframe had no salary info 
sparkdf = sparkdf.fillna({'salary': 'No salary info'})
sparkdf.filter(sparkdf["salary"] == 'No salary info').count()/sparkdf.count() * 100

#Roughly 11.19% of all jobs are classified as Remote
sparkdf.filter(sparkdf["location"]=="Remote").count()/sparkdf.count() * 100

#Roughly 23.37% of all Remote jobs have no salary info
sparkdf.filter((sparkdf["location"] == "Remote") & (sparkdf["salary"] == "No salary info")).count()/sparkdf.filter(sparkdf["location"] == "Remote").count()*100

"""# Final Conclusion

Post completing analysis of the data frames, there are a few notable insights that emerged in regards to what to expect when applying for jobs requiring data analysis skills. 

-The areas of the United States with the most job openings are the east and west coasts. This is largely due to the fact that tech industries tend to be more prominent in these areas.

-Many of these jobs do not have the 'easy apply' option if you were to apply on Linkedin. That means that the process to get hired is extensive. We also found it to be true that a majority of the jobs are not remote and are full time. 

-One of the companies with the most job openings listed is Discover. 

-The most popular job titles include data science lead, data science visualization engineer, lead data science analyst, and data science intern. 

-The words that appear the most in the job titles are 'Data Science'. Even though some of the titles have other terms such as 'engineer','lead', and 'analyst'; the common thread is the concept that you will be working in the field of data science. 

-In one of the largest files we worked with, roughly 73% of the job postings had no salary information. 23% of the remote jobs did not have salary information. So, even though there is not a considerable amount of jobs with salary information, you are more likely to find one with information if it is not remote.

# Presentation plans

Quick Discussion of the cleaning process:

**Final Conclusion portion discussion**

1. -The areas of the United States with the most job openings are the east and west coasts. This is largely due to the fact that tech industries tend to be more prominent in these areas.

2. -Many of these jobs do not have the 'easy apply' option if you were to apply on Linkedin. That means that the process to get hired is extensive. We also found it to be true that a majority of the jobs are not remote and are full time

3. -One of the companies with the most job openings listed is Discover.

4. -The most popular job titles include data science lead, data science visualization engineer, lead data science analyst, and data science intern.

5. -The words that appear the most in the job titles are 'Data Science'. Even though some of the titles have other terms such as 'engineer','lead', and 'analyst'; the common thread is the concept that you will be working in the field of data science.

6. -In one of the largest files we worked with, roughly 73% of the job postings had no salary information. 23% of the remote jobs did not have salary information. So, even though there is not a considerable amount of jobs with salary information, you are more likely to find one with information if it is not remote.
"""